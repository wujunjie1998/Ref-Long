{"0": ["Yi: Open Foundation Models by 01.AI", [2, 5]], "1": ["EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models", [3, 4]], "2": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [0, 6]], "3": ["A Survey on Large Language Model-Based Game Agents", [3, 5]], "4": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [3, 7]], "5": ["InternLM2 Technical Report", [1, 2, 5, 6]], "6": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [0, 3]], "7": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 3, 5, 6]], "8": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [4, 5]], "9": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [1, 4]], "10": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [0, 3, 6]], "11": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [6, 7]], "12": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [2, 6, 7]], "13": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [3, 6]], "14": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [1, 2, 4]], "15": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [0, 4, 5, 7]], "16": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [1, 3, 4, 6]], "17": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [1, 2]], "18": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [4, 5]], "19": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 2, 4]], "20": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [3, 6]], "21": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [0, 1, 7]], "22": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [0, 4]], "23": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 3, 7]], "24": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [0, 1, 2, 6]], "25": ["Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study", [2, 3]], "26": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [6, 7]], "27": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 2, 4]], "28": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [3, 4]], "29": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [0, 4]], "30": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 2, 4, 5, 7]], "31": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [3, 4]], "32": ["BadEdit: Backdooring Large Language Models by Model Editing", [4, 6]], "33": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [2, 3]], "34": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [0, 2, 3]], "35": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [1, 4, 6]], "36": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [0, 1, 4, 5]], "37": ["Mapping the Increasing Use of LLMs in Scientific Papers", [2, 5, 6]], "38": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [3, 5, 6]], "39": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [2, 5, 7]], "40": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 2, 3, 4]], "41": ["LLM Evaluators Recognize and Favor Their Own Generations", [1, 3, 4, 6, 7]], "42": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [0, 3]], "43": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [2, 5]], "44": ["ALOHa: A New Measure for Hallucination in Captioning Models", [2, 6]], "45": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [3, 6, 7]], "46": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [1, 2, 3, 5, 7]], "47": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 2, 4]], "48": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [0, 1, 7]], "49": ["InternLM2 Technical Report", [1, 2, 6, 7]], "50": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [0, 1, 7]], "51": ["InsectMamba: Insect Pest Classification with State Space Model", [1, 5]], "52": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 2, 3, 7]], "53": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [1, 3, 6]], "54": ["A Multimodal Automated Interpretability Agent", [1, 3]], "55": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 2, 4]], "56": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 5, 6, 7]], "57": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [5, 7]], "58": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 2, 3]], "59": ["Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", [1, 3, 5]], "60": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [2, 5]], "61": ["Measuring Political Bias in Large Language Models: What Is Said and How It Is Said", [1, 7]], "62": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [0, 7]], "63": ["Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", [0, 4, 7]], "64": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [0, 2, 4]], "65": ["A Multimodal Automated Interpretability Agent", [6, 7]], "66": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [3, 6]], "67": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 5, 6, 7]], "68": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [2, 5]], "69": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [0, 3]], "70": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 4, 7]], "71": ["BadEdit: Backdooring Large Language Models by Model Editing", [1, 5]], "72": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [1, 6]], "73": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [3, 7]], "74": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [2, 5]], "75": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 2, 3, 6, 7]], "76": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [0, 1, 2, 7]], "77": ["ALOHa: A New Measure for Hallucination in Captioning Models", [1, 7]], "78": ["InternLM2 Technical Report", [0, 1, 4, 6]], "79": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [0, 2, 6, 7]], "80": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [1, 6]], "81": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [2, 3]], "82": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [0, 2, 3]], "83": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [1, 6]], "84": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [2, 5]], "85": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [1, 2, 3, 4, 6]], "86": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [0, 5]], "87": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [4, 5, 6]], "88": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [2, 4, 5]], "89": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [0, 2]], "90": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [0, 3]], "91": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 1, 2, 3, 6]], "92": ["Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?", [1, 4, 7]], "93": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [3, 7]], "94": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [0, 5]], "95": ["ALOHa: A New Measure for Hallucination in Captioning Models", [3, 4]], "96": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 2, 6]], "97": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [2, 5, 7]], "98": ["A Survey on Large Language Model-Based Game Agents", [1, 4]], "99": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [3, 7]]}