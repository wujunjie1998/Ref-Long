{"0": ["Yi: Open Foundation Models by 01.AI", [2, 14]], "1": ["EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models", [0, 7]], "2": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [5, 8]], "3": ["A Survey on Large Language Model-Based Game Agents", [13, 19]], "4": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [2, 4]], "5": ["InternLM2 Technical Report", [3, 7, 12, 19]], "6": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [7, 8]], "7": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 3, 11, 17]], "8": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [2, 8]], "9": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [2, 16]], "10": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [1, 7, 10]], "11": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [1, 10]], "12": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [3, 5, 7]], "13": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [7, 8]], "14": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [4, 14, 16]], "15": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [0, 3, 16, 18]], "16": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [1, 4, 9, 10]], "17": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [7, 15]], "18": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [2, 15]], "19": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 10, 17]], "20": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [8, 13]], "21": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [2, 4, 11]], "22": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [4, 15]], "23": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [1, 4, 16]], "24": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [0, 3, 4, 10]], "25": ["Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study", [0, 12]], "26": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [6, 17]], "27": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [1, 7, 16]], "28": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [0, 16]], "29": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [14, 15]], "30": ["LLM Evaluators Recognize and Favor Their Own Generations", [1, 2, 12, 13, 14]], "31": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [2, 5]], "32": ["BadEdit: Backdooring Large Language Models by Model Editing", [3, 10]], "33": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [0, 11]], "34": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [4, 10, 13]], "35": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [2, 4, 18]], "36": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [14, 16, 17, 18]], "37": ["Mapping the Increasing Use of LLMs in Scientific Papers", [14, 18, 19]], "38": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [0, 1, 14]], "39": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [1, 6, 17]], "40": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [0, 3, 6, 19]], "41": ["LLM Evaluators Recognize and Favor Their Own Generations", [5, 6, 10, 16, 19]], "42": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [11, 19]], "43": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [17, 18]], "44": ["ALOHa: A New Measure for Hallucination in Captioning Models", [8, 16]], "45": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [2, 8, 9]], "46": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [5, 8, 11, 12, 15]], "47": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 5, 16]], "48": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [3, 4, 12]], "49": ["InternLM2 Technical Report", [7, 8, 9, 16]], "50": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [7, 8, 10]], "51": ["InsectMamba: Insect Pest Classification with State Space Model", [5, 15]], "52": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 5, 15, 16]], "53": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [7, 9, 11]], "54": ["A Multimodal Automated Interpretability Agent", [7, 13]], "55": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [7, 11, 18]], "56": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 7, 16, 18]], "57": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [13, 17]], "58": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 7, 8]], "59": ["Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", [2, 4, 8]], "60": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [12, 17]], "61": ["Measuring Political Bias in Large Language Models: What Is Said and How It Is Said", [12, 19]], "62": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [9, 11]], "63": ["Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", [10, 14, 17]], "64": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [4, 8, 10]], "65": ["A Multimodal Automated Interpretability Agent", [6, 17]], "66": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [11, 14]], "67": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 3, 11, 16]], "68": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [6, 12]], "69": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [5, 19]], "70": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [1, 16, 19]], "71": ["BadEdit: Backdooring Large Language Models by Model Editing", [6, 13]], "72": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [7, 19]], "73": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [3, 7]], "74": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [0, 13]], "75": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 1, 3, 15, 16]], "76": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [2, 3, 9, 10]], "77": ["ALOHa: A New Measure for Hallucination in Captioning Models", [0, 9]], "78": ["InternLM2 Technical Report", [2, 4, 7, 14]], "79": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [5, 9, 13, 14]], "80": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [0, 7]], "81": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [5, 12]], "82": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [12, 14, 17]], "83": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [3, 17]], "84": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [5, 7]], "85": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [4, 5, 7, 12, 19]], "86": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [11, 12]], "87": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [1, 8, 16]], "88": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [3, 15, 18]], "89": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [1, 18]], "90": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [1, 9]], "91": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 1, 4, 6, 14]], "92": ["Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?", [0, 3, 9]], "93": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [2, 9]], "94": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [4, 16]], "95": ["ALOHa: A New Measure for Hallucination in Captioning Models", [5, 15]], "96": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 3, 5]], "97": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [11, 14, 19]], "98": ["A Survey on Large Language Model-Based Game Agents", [1, 6]], "99": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [1, 15]]}