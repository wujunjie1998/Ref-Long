{"0": ["Yi: Open Foundation Models by 01.AI", [0, 9]], "1": ["EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models", [2, 7]], "2": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [7, 10]], "3": ["A Survey on Large Language Model-Based Game Agents", [7, 13]], "4": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [0, 15]], "5": ["InternLM2 Technical Report", [0, 4, 8, 9]], "6": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [2, 15]], "7": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [4, 9, 11, 12]], "8": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [9, 10]], "9": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [6, 8]], "10": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [1, 2, 8]], "11": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [1, 4]], "12": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [3, 5, 12]], "13": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [7, 9]], "14": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [3, 6, 14]], "15": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 7, 10, 11]], "16": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [4, 6, 10, 11]], "17": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [1, 7]], "18": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [4, 7]], "19": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [3, 13, 14]], "20": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [0, 13]], "21": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [3, 6, 7]], "22": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [6, 7]], "23": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [3, 8, 13]], "24": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [3, 6, 12, 14]], "25": ["Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study", [0, 11]], "26": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [12, 13]], "27": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [11, 12, 13]], "28": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [3, 11]], "29": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [7, 12]], "30": ["LLM Evaluators Recognize and Favor Their Own Generations", [1, 2, 6, 8, 15]], "31": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [1, 3]], "32": ["BadEdit: Backdooring Large Language Models by Model Editing", [1, 14]], "33": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [2, 5]], "34": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [2, 3, 13]], "35": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 7, 8]], "36": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [8, 12, 13, 15]], "37": ["Mapping the Increasing Use of LLMs in Scientific Papers", [2, 5, 7]], "38": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [10, 12, 13]], "39": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [1, 2, 11]], "40": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 8, 9, 14]], "41": ["LLM Evaluators Recognize and Favor Their Own Generations", [9, 10, 11, 13, 15]], "42": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [7, 10]], "43": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [2, 15]], "44": ["ALOHa: A New Measure for Hallucination in Captioning Models", [4, 13]], "45": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [1, 3, 12]], "46": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [1, 3, 7, 10, 13]], "47": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [1, 7, 10]], "48": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [1, 7, 11]], "49": ["InternLM2 Technical Report", [0, 7, 9, 14]], "50": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [6, 8, 12]], "51": ["InsectMamba: Insect Pest Classification with State Space Model", [7, 9]], "52": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [0, 2, 8, 11]], "53": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [1, 3, 13]], "54": ["A Multimodal Automated Interpretability Agent", [7, 14]], "55": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 8, 9]], "56": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 6, 10, 14]], "57": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [10, 15]], "58": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [4, 6, 13]], "59": ["Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", [0, 2, 3]], "60": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [8, 12]], "61": ["Measuring Political Bias in Large Language Models: What Is Said and How It Is Said", [0, 15]], "62": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [0, 5]], "63": ["Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", [1, 6, 15]], "64": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [5, 11, 15]], "65": ["A Multimodal Automated Interpretability Agent", [14, 15]], "66": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [5, 10]], "67": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 4, 9, 10]], "68": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [10, 12]], "69": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [5, 7]], "70": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [7, 9, 14]], "71": ["BadEdit: Backdooring Large Language Models by Model Editing", [8, 15]], "72": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [9, 12]], "73": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [3, 5]], "74": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [4, 8]], "75": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 3, 7, 8, 9]], "76": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [1, 7, 8, 14]], "77": ["ALOHa: A New Measure for Hallucination in Captioning Models", [2, 9]], "78": ["InternLM2 Technical Report", [6, 11, 12, 13]], "79": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [3, 4, 6, 9]], "80": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [4, 14]], "81": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [10, 15]], "82": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [1, 12, 14]], "83": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [4, 8]], "84": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [5, 12]], "85": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [1, 2, 8, 11, 14]], "86": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [9, 14]], "87": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 6, 9]], "88": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [8, 9, 10]], "89": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [10, 13]], "90": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [10, 14]], "91": ["LLM Evaluators Recognize and Favor Their Own Generations", [3, 4, 5, 11, 13]], "92": ["Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?", [6, 8, 9]], "93": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [2, 15]], "94": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [0, 14]], "95": ["ALOHa: A New Measure for Hallucination in Captioning Models", [1, 7]], "96": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [2, 3, 8]], "97": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [5, 11, 14]], "98": ["A Survey on Large Language Model-Based Game Agents", [0, 6]], "99": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [9, 14]]}