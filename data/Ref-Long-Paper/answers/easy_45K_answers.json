{"0": ["Yi: Open Foundation Models by 01.AI", [6, 11]], "1": ["EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models", [0, 7]], "2": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [0, 4]], "3": ["A Survey on Large Language Model-Based Game Agents", [2, 8]], "4": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [7, 8]], "5": ["InternLM2 Technical Report", [0, 1, 8, 11]], "6": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [2, 4]], "7": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 3, 8, 9]], "8": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [0, 9]], "9": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [7, 10]], "10": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [2, 7, 8]], "11": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [3, 9]], "12": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [4, 5, 7]], "13": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [1, 2]], "14": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [1, 3, 6]], "15": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 4, 5, 9]], "16": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [1, 2, 3, 7]], "17": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [2, 9]], "18": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [2, 6]], "19": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [0, 1, 8]], "20": ["Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", [2, 10]], "21": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [2, 3, 11]], "22": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [8, 11]], "23": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [4, 5, 7]], "24": ["Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models", [1, 4, 9, 10]], "25": ["Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study", [9, 11]], "26": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [5, 9]], "27": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [2, 6, 7]], "28": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [8, 10]], "29": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [2, 7]], "30": ["LLM Evaluators Recognize and Favor Their Own Generations", [1, 3, 9, 10, 11]], "31": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [5, 8]], "32": ["BadEdit: Backdooring Large Language Models by Model Editing", [4, 6]], "33": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [10, 11]], "34": ["Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", [6, 7, 11]], "35": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [3, 9, 11]], "36": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [0, 1, 4, 11]], "37": ["Mapping the Increasing Use of LLMs in Scientific Papers", [1, 2, 3]], "38": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [4, 6, 10]], "39": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [1, 4, 9]], "40": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 5, 6, 8]], "41": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 1, 4, 5, 6]], "42": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [4, 10]], "43": ["Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon", [1, 7]], "44": ["ALOHa: A New Measure for Hallucination in Captioning Models", [4, 5]], "45": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [3, 9, 10]], "46": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [0, 6, 7, 8, 10]], "47": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [2, 6, 11]], "48": ["Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation", [8, 10, 11]], "49": ["InternLM2 Technical Report", [3, 4, 5, 7]], "50": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [1, 2, 11]], "51": ["InsectMamba: Insect Pest Classification with State Space Model", [2, 11]], "52": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [5, 8, 10, 11]], "53": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [1, 4, 8]], "54": ["A Multimodal Automated Interpretability Agent", [2, 9]], "55": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [2, 7, 8]], "56": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [2, 6, 8, 9]], "57": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [1, 10]], "58": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [2, 3, 6]], "59": ["Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", [0, 2, 11]], "60": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [9, 10]], "61": ["Measuring Political Bias in Large Language Models: What Is Said and How It Is Said", [2, 7]], "62": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [5, 6]], "63": ["Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", [1, 4, 11]], "64": ["Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", [0, 2, 6]], "65": ["A Multimodal Automated Interpretability Agent", [10, 11]], "66": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [2, 10]], "67": ["Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", [1, 7, 10, 11]], "68": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [2, 10]], "69": ["SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs", [9, 10]], "70": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [0, 6, 7]], "71": ["BadEdit: Backdooring Large Language Models by Model Editing", [2, 8]], "72": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [4, 9]], "73": ["RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation", [9, 11]], "74": ["FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication", [4, 6]], "75": ["LLM Evaluators Recognize and Favor Their Own Generations", [0, 4, 5, 7, 8]], "76": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [1, 5, 6, 9]], "77": ["ALOHa: A New Measure for Hallucination in Captioning Models", [0, 5]], "78": ["InternLM2 Technical Report", [1, 3, 8, 10]], "79": ["SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials", [1, 4, 6, 9]], "80": ["Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance", [0, 9]], "81": ["LoRA Dropout as a Sparsity Regularizer for Overfitting Control", [2, 11]], "82": ["On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods", [7, 8, 10]], "83": ["From Local to Global: A Graph RAG Approach to Query-Focused Summarization", [4, 5]], "84": ["ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", [8, 11]], "85": ["SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", [2, 5, 9, 10, 11]], "86": ["Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization", [9, 11]], "87": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [3, 6, 11]], "88": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [1, 4, 11]], "89": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [3, 4]], "90": ["Direct Nash Optimization: \u200b\u200bTeaching Language Models to Self-Improve with General Preferences", [1, 3]], "91": ["LLM Evaluators Recognize and Favor Their Own Generations", [2, 3, 8, 10, 11]], "92": ["Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?", [2, 5, 7]], "93": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [7, 9]], "94": ["Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal", [2, 10]], "95": ["ALOHa: A New Measure for Hallucination in Captioning Models", [10, 11]], "96": ["VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?", [1, 9, 11]], "97": ["From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", [2, 3, 8]], "98": ["A Survey on Large Language Model-Based Game Agents", [3, 7]], "99": ["Don\u2019t Trust: Verify \u2013 Grounding LLM Quantitative Reasoning with Autoformalization", [6, 9]]}